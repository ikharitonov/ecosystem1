{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297d1127-054e-4c14-b7d1-8f965927ef10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from pypdf import PdfReader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58343163-31c8-4b7f-a35a-1fab237d7010",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter-ikharitonov/code/ecosystem1/../../pdfs/Projects_to_candidates.pdf')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_folder = '../../pdfs'\n",
    "pdf_path = Path(pdf_folder) / os.listdir(pdf_folder)[2]\n",
    "pdf_path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f440b6aa-5317-4bce-9be8-f7a70ec4dd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = PdfReader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835ba6c5-590e-4e08-8375-897344086ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for page in reader.pages:\n",
    "    # page_text = page.extract_text()\n",
    "    # page_text = page_text.replace('\\n', '')\n",
    "    # print(page.extract_text())\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be14ae49-e975-4479-9fe9-92e44e4ae06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_chunk_char = 10\n",
    "max_chunk_char = 200\n",
    "\n",
    "def stupid_chunking(text):\n",
    "    chunks = []\n",
    "    lower_bound = 0\n",
    "    for i in range(len(text)//max_chunk_char):\n",
    "        higher_bound = lower_bound + max_chunk_char\n",
    "        chunks.append(text[lower_bound:higher_bound])\n",
    "        lower_bound += max_chunk_char\n",
    "    return chunks\n",
    "\n",
    "chunks = stupid_chunking(' '.join([x.extract_text() for x in reader.pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e7b93d-f44b-4763-8506-048f60095e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    \\n2022 Rancz Lab Projects    CONFIDENTIAL 1 Research Project – Predictive processing in the mouse cortex  Introduction  To understand how the brain works, we need hypotheses about what it is doing.',\n",
       " ' Historically, the brain was treated as feedforward processing device, based initially on the connectivity diagram of cortical visual areas compiled in 1991 by Felleman and Van Essen1. One alternative',\n",
       " ' framework recently gaining prominence is the so-called predictive processing theory2. It postulates that brains constantly attempt to match bottom-up sensory input with top-down, internally generated',\n",
       " ' predictions3-5. Consequently, the most common neuronal computation across many brain areas is processing prediction error signals6. In terms of implementing such a coding strategy, several algorithms',\n",
       " ' have been proposed7, however, they lack solid biological support. Similarly, despite efforts to uncover neuronal elements and circuit motifs supporting the computations needed for such working mode8-',\n",
       " '10, there is no clear consensus nor a reliable biological model system to study the mechanistic underpinnings of predictive coding11,12. Over the last few years, my laboratory has established a model ',\n",
       " 'system rooted in single-cell connectivity and physiology to study predictive processing. Our research focuses on layer 5 pyramidal neurons (L5PN) of the mouse secondary visual cortex (V2M) since they ',\n",
       " 'receive diverse inputs and provide the main output of cortical computations. We use a transgenic mouse line with specific cre expression in a small group of such neurons (Colgalt-2)13. First, we used ',\n",
       " 'rabies virus-mediated tracing to generate whole-brain input maps to L5PNs14. We revealed bottom-up information streams (visual and vestibular) and top-down inputs from higher brain areas associated wi',\n",
       " 'th internal models of space (retrosplenial cortex, RSP15), self-generated movement (anterior cingulate area, ACA16) or task parameters and reward value (orbitofrontal cortex, ORB17,18). Next, we combi',\n",
       " 'ned patch-clamp recordings with computational modelling to examine how the different inputs interact to generate action potential output in V2M L5PNs. We discovered that global coincidence detection m',\n",
       " 'echanisms assumed to be ubiquitous in integrating bottom-up and top-down information are lacking in these neurons, partially due to their morphology19. Finally, we have measured the functional strengt',\n",
       " 'h and mapped the dendritic location of the bottom-up and top-down inputs identified using rabies tracing14 and found unexpectedly powerful synaptic input from some top-down input areas (e.g. ACA, ORB)',\n",
       " '. In parallel with studying the input to L5PNs, we used axonal tracing to identify relevant subcortical output regions. We found abundant projections to the deep layers of the superior colliculus, a s',\n",
       " 'tructure orchestrating orienting movements of the head and the eyes. Following this lead, we have shown that optogenetic interference with only a handful of L5PNs in V2M can disorganise eye-movements ',\n",
       " 'during combined vestibular and visual stimulation in awake mice (manuscript in preparation, see Figure 2).  In summary, we have established V2M L5PNs as a model system with bottom-up inputs (visual, v',\n",
       " 'estibular) and top-down inputs from internal model related areas (physical and abstract spaces) as well as a systemic output (eye-movements). We have two main work packages for the next 5 years. First',\n",
       " ', to control bottom-up inputs to interrogate the role of top-down activity in innate and learned behavioural tasks. To this end, we have developed a novel, open-source research apparatus. We will comb',\n",
       " 'ine electrophysiological recordings in head-fixed, awake mice with visual virtual reality and animal-driven vestibular rotation (manuscript in preparation, see Figure 1), enabling us to introduce sens',\n",
       " 'ory prediction errors during behaviour. The second work package expands on our previous work on the integration of long-range excitatory inputs. We will examine the contribution of disynaptic, feedfor',\n",
       " 'ward inhibition driven by top-down input and the influence of neuromodulators. A particular long-term goal of my team is to open up this model system to experiments directly relevant to psychiatric di',\n",
       " 'seases, as discussed in work package 2.  Work Package 1. The role of top-down signalling in behaviour  This work package builds on our previous work in identifying top-down inputs to L5PNs and develop',\n",
       " 'ing the vestibulo-visual platform. We will focus on the contribution of RSP, ACA and ORB to the functioning of V2M. The main objectives are: to further develop the experimental setup, to establish inn',\n",
       " 'ate and learned behavioural tasks, and to record and interfere with neuronal activity during task acquisition and execution.       \\n2022 Rancz Lab Projects    CONFIDENTIAL 2 Aim 1. Vestibulo-visual pl',\n",
       " 'atform development  The current working prototype of the vestibulo-visual platform (Figure 1) utilises disparate systems for motor-control (Teensy microcontroller), visual stimulation (Raspberry Pi), ',\n",
       " 'eye-movement videography and silicone probe recordings (computer). We will implement several improvements to enable high throughput recording and fast behavioural prototyping necessary for our goals. ',\n",
       " 'First, we will integrate the vestibulo-visual platform with neuropixels 2.0 extracellular recording electrodes20 and the next-generation open-source acquisition system ONIX (https://open-ephys.org/nex',\n",
       " 't-gen-acquisition-system). This will allow recording from hundreds of neurons simultaneously across the visual cortex. Crucially, experimental control (e.g. reward delivery, optogenetic activation), s',\n",
       " 'ensory stimulation (vestibular control, visual stimuli and virtual reality21), eye-movement videography22 and extracellular recordings will be brought together under the reactive computing environment',\n",
       " ', Bonsai23 (https://bonsai-rx.org/). System integration and specific developments required for motor control will be carried out in collaboration with Dr Gonçalo Lopes and NeuroGears, a scientific eng',\n",
       " 'ineering company. Dr Lopes is the lead developer of Bonsai and is best placed to carry out this work. Funding has already been secured for the required new hardware and for consultation and bespoke de',\n",
       " 'velopment fees. The platform will be shared with the research community as an open-source asset.  \\n Figure 1. Vestibular platform. (A) High-level I/O, (B) Algorithmic implementation and (C) Device, CA',\n",
       " 'D model (left) and photograph (right).   Aim 2. Simple behaviour  We will first study the effects of top-down inputs using an innate behaviour, which does not require lengthy task acquisition. We will',\n",
       " ' focus on eye-movements stabilizing the retinal image during self- and external movement. In particular, the optokinetic reflex driven by external motion has been shown to be modulated by the visual c',\n",
       " 'ortex24. Furthermore, top-down input from RSP has been shown to relay rotational movement information to the visual cortex25, and we will engage this using passive vestibular rotation. On the other ha',\n",
       " 'nd, predictions of visual flow based on self-generated movement reach the visual cortex via ACA16. We will engage this pathway by active vestibular rotation (i.e., rotation generated by the animal).  ',\n",
       " 'Specifically, we will carry out acute recordings in V2M using neuropixels probes while presenting congruent and conflicting vestibular and visual motion. Pilot data from my laboratory show that interf',\n",
       " 'ering with the \\n     \\n2022 Rancz Lab Projects    CONFIDENTIAL 3 activity of only a few hundred L5PNs in V2M has a profound effect on eye-movements during combined visual and vestibular rotation (Figur',\n",
       " 'e 2 A-C). We hypothesize that input from RSP predicts visual movement based on rotational movement information and conflicting visual movements will result in a prediction error. We will test this in ',\n",
       " 'two complementary ways, either by optogenetically inactivating bottom-up information carried by local layer 2/3 inputs using a cre-off viral approach, or by optogenetically inactivating top-down RSP n',\n",
       " 'eurons targeting V2M using a rabies virus-based approach while recording eye-movements and single-unit activity in V2M. We will expand these experiments to self-generated movement and visual-flow pred',\n",
       " 'ictions arising from ACA in a similar fashion.  \\n Figure 2. Eye-movement recording and visual virtual reality on the vestibular platform. A. 4 monitors surround the vestibular platform to create a vir',\n",
       " 'tual drum (i.e., moving gratings) to evoke a combination of optokinetic and vestibulo-ocular reflexes while the animal is rotated. B. Eye-movements are monitored through a camera and analysed using De',\n",
       " 'epLabCut. C. The optogenetic interference with a few hundred L5PNs in V2M results in disorganized eye-movements during combined vestibular and visual stimulation. D. Movie frame showing a mouse naviga',\n",
       " 'ting on the vestibular platform. Importantly, visual virtual reality is combined with self-generated rotation and the 2D trajectory of the animal is shown by the red trace.   Aim 3. Complex behaviour ',\n",
       " ' Next, we will develop a learned behavioural task involving eye-movements. The innate eye-movements we will investigate in Aim 2 consist of a slow pursuit phase followed by a fast “reset” saccade (Fig',\n",
       " 'ure 2C). However, these reflexive saccades are not the only fast gaze shifts mice can do. Increasing evidence supports the presence of stimulus driven, fast targeted eye-movements in mice26,27. Simila',\n",
       " 'rly, slow pursuit eye movements driven by the movement of the whole visual field can likely be modulated by top-down cortical activity to focus on target objects. As mice have been shown capable of co',\n",
       " 'mplex, highly visual tasks such as hunting crickets28, we will develop a virtual hunting task that will require spatial gaze targeting both on fast and slow time scales.  Animals will be kept under a ',\n",
       " 'water restriction regime and task acquisition will be facilitated by water rewards. Probabilistic rewards will be used to engage the orbitofrontal cortex29,30. Given the complexity of such a task, we ',\n",
       " 'will employ a step-wise development. 1. Directed gaze shifts towards a stationary visual target.  2. Navigation to a stationary visual target using the vestibular platform in closed-loop (i.e. the rot',\n",
       " 'ation of the platform is driven by the movement of the animal, Figure 2D). If necessary, we will facilitate learning this task by engaging the recently described opto-locomotor reflex31.  3. A virtual',\n",
       " ' cricket hunt. This task will require both smooth pursuit of moving visual targets and fast targeted gaze shifts for “jumping” targets. One of the advantages of this complex approach is that it natura',\n",
       " 'lly involves visual prediction errors.  \\n     \\n2022 Rancz Lab Projects    CONFIDENTIAL 4 We will follow the same recording and optogenetic interference strategies describe in Aim 2 using acute recordi',\n",
       " 'ngs in trained animals. A future expansion of this research project will use chronic recordings from relevant top-down areas during task acquisition to follow the development of internal models during',\n",
       " ' learning.  Work Package 2: Inhibitory and neuromodulatory mechanisms for predictive processing in V2M There are multiple mechanisms suggested for integrating bottom-up and top-down inputs at the leve',\n",
       " 'l of single neurons32 and the local network2. To uncover and understand the mechanical substrates of predictive processing, we will expand our previous work on subcellular input localization and singl',\n",
       " 'e-cell synaptic integration. We will focus on the most powerful bottom-up (local input from L2/3) and top-down inputs (RSP, ACA, ORB).  Aim 1: The contribution of feedforward inhibition to predictive ',\n",
       " 'processing  It has been suggested that prediction errors are computed in two separate circuits, signalling either more or less input than predicted33. The critical difference between these two hypothe',\n",
       " 'sized circuits is whether there is feedforward inhibition in the pathway or not2. To test this hypothesis directly, we will express the optogenetic activator Chronos in top-down areas RSP, ORB and ACA',\n",
       " '. Consequently, we will record optogenetically evoked synaptic potentials from L5PN under physiological conditions (i.e. no synaptic receptor or ion-channel blockers). Evoked responses will thus be ei',\n",
       " 'ther purely excitatory or a combination of excitation followed by disynaptic, feedforward inhibition, thus categorizing input pathways as carrying positive or negative prediction error. However, it is',\n",
       " ' possible that individual pathways carry a mixture of positive and negative prediction error signals. We will test for this possible heterogeny by minimal stimulation34 in different spatial locations ',\n",
       " 'using a digital micromirror device already available in my laboratory.  Aim 2. The contribution of serotonergic neuromodulation to predictive processing Neuromodulator systems can alter the balance of',\n",
       " ' top-down versus bottom-up influence35, shifting the relative contribution of bottom-up and top-down signals and determining the extent to which bottom-up inputs are used to update the internal model.',\n",
       " ' It can be argued that positive symptoms in psychotic diseases such as schizophrenia or the manic phase of bipolar disorder (i.e., hallucinations) are a consequence of top-down, predictive inputs domi',\n",
       " 'nating sensory perception36,37. We will specifically focus on serotonin receptors, as many psychotomimetic drugs (e.g. LSD, psilocybin) preferentially activate 5-HT2A receptors to induce visual halluc',\n",
       " 'inations38. Activation of these receptors has been recently shown to reduce visual response gain in the primary visual cortex of macaques39 and mice40. In addition, there seems to be a segregated effe',\n",
       " 'ct of 5-HT2A and 5-HT1A receptors on network activity driven by top-down and bottom-up input41. Importantly, 5-HT2A receptors have been shown to tune dendritic integration in rat visual cortex L5PNs b',\n",
       " 'oth directly and indirectly through dendritic-targeting interneurons42.  First, we will record from L5PNs in acute slices and test the effect of the selective 5-HT2A receptor agonist DOI (2,5-dimethox',\n",
       " 'y-4-iodoamphetamine) on compound excitatory and inhibitory responses evoked by top-down input identified in Aim 1. Second, we will use the behavioural task developed in WP1 Aim 2 (simple behaviour) to',\n",
       " ' test the effect of locally or systemically applied DOI on population activity in V2M and eye-movement responses. Importantly, these experiments will provide a well-defined model system to study netwo',\n",
       " 'rk mechanisms in acute psychosis and the ameliorating effect of modern atypical antipsychotics, many of which target the serotonergic system.       \\n2022 Rancz Lab Projects    CONFIDENTIAL 5 Reference',\n",
       " 's 1 Felleman, D. J. & Van Essen, D. C. Distributed hierarchical processing in the primate cerebral cortex. Cereb Cortex 1, 1-47 (1991). 2 Keller, G. B. & Mrsic-Flogel, T. D. Predictive Processing: A C',\n",
       " 'anonical Cortical Computation. Neuron 100, 424-435, doi:10.1016/j.neuron.2018.10.003 (2018). 3 Bar, M. Predictions: a universal principle in the operation of the human brain. Introduction. Philos Tran',\n",
       " 's R Soc Lond B Biol Sci 364, 1181-1182, doi:10.1098/rstb.2008.0321 (2009). 4 Friston, K. A theory of cortical responses. Philos Trans R Soc Lond B Biol Sci 360, 815-836, doi:10.1098/rstb.2005.1622 (20',\n",
       " '05). 5 Hohwy, J. The predictive mind. First edition. edn,  (Oxford University Press, 2013). 6 den Ouden, H. E., Kok, P. & de Lange, F. P. How prediction errors shape perception, attention, and motivat',\n",
       " 'ion. Front Psychol 3, 548, doi:10.3389/fpsyg.2012.00548 (2012). 7 Spratling, M. W. A review of predictive coding algorithms. Brain Cogn 112, 92-97, doi:10.1016/j.bandc.2015.11.003 (2017). 8 Adams, R. ',\n",
       " 'A., Shipp, S. & Friston, K. J. Predictions not commands: active inference in the motor system. Brain Struct Funct 218, 611-643, doi:10.1007/s00429-012-0475-5 (2013). 9 Bastos, A. M. et al. Canonical m',\n",
       " 'icrocircuits for predictive coding. Neuron 76, 695-711, doi:10.1016/j.neuron.2012.10.038 (2012). 10 Shipp, S. Neural Elements for Predictive Coding. Front Psychol 7, 1792, doi:10.3389/fpsyg.2016.01792',\n",
       " ' (2016). 11 Egner, T. & Summerfield, C. Grounding predictive coding models in empirical neuroscience research. Behav Brain Sci 36, 210-211, doi:10.1017/S0140525X1200218X (2013). 12 Kogo, N. & Trengove',\n",
       " ', C. Is predictive coding theory articulated enough to be testable? Front Comput Neurosci 9, 111, doi:10.3389/fncom.2015.00111 (2015). 13 Groh, A. et al. Cell-type specific properties of pyramidal neu',\n",
       " 'rons in neocortex underlying a layout that is modifiable depending on the cortical area. Cereb Cortex 20, 826-836, doi:10.1093/cercor/bhp152 (2010). 14 Galloni, A. R., Ye, Z. & Rancz, E. Dendritic dom',\n",
       " 'ain-specific sampling of long-range axons shapes feedforward and feedback connectivity of L5 neurons. accepted in Journal of Neuroscience, doi:10.1101/2021.01.31.429033 (2022). 15 Vann, S. D., Aggleto',\n",
       " 'n, J. P. & Maguire, E. A. What does the retrosplenial cortex do? Nat Rev Neurosci 10, 792-802, doi:10.1038/nrn2733 (2009). 16 Leinweber, M., Ward, D. R., Sobczak, J. M., Attinger, A. & Keller, G. B. A',\n",
       " ' Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions. Neuron 95, 1420-1432 e1425, doi:10.1016/j.neuron.2017.08.036 (2017). 17 Feierstein, C. E., Quirk, M. C., Uchida, N., Sosulski, D. L. ',\n",
       " '& Mainen, Z. F. Representation of spatial goals in rat orbitofrontal cortex. Neuron 51, 495-507, doi:10.1016/j.neuron.2006.06.032 (2006). 18 Rudebeck, P. H. & Murray, E. A. The orbitofrontal oracle: c',\n",
       " 'ortical mechanisms for the prediction and evaluation of specific behavioral outcomes. Neuron 84, 1143-1156, doi:10.1016/j.neuron.2014.10.049 (2014). 19 Galloni, A. R., Laffere, A. & Rancz, E. Apical l',\n",
       " 'ength governs computational diversity of layer 5 pyramidal neurons. Elife 9, doi:10.7554/eLife.55761 (2020). 20 Steinmetz, N. A. et al. Neuropixels 2.0: A miniaturized high-density probe for stable, l',\n",
       " 'ong-term brain recordings. Science 372, doi:10.1126/science.abf4588 (2021). 21 Lopes, G. et al. Creating and controlling visual environments using BonVision. Elife 10, doi:10.7554/eLife.65541 (2021). ',\n",
       " '22 Kane, G. A., Lopes, G., Saunders, J. L., Mathis, A. & Mathis, M. W. Real-time, low-latency closed-loop feedback using markerless posture tracking. Elife 9, doi:10.7554/eLife.61909 (2020). 23 Lopes,',\n",
       " ' G. et al. Bonsai: an event-based framework for processing and controlling data streams. Front Neuroinform 9, 7, doi:10.3389/fninf.2015.00007 (2015). 24 Liu, B. H., Huberman, A. D. & Scanziani, M. Cor',\n",
       " 'tico-fugal output from visual cortex promotes plasticity of innate motor behaviour. Nature 538, 383-387, doi:10.1038/nature19818 (2016). 25 Velez-Fort, M. et al. A Circuit for Integration of Head- and',\n",
       " \" Visual-Motion Signals in Layer 6 of Mouse Primary Visual Cortex. Neuron 98, 179-191 e176, doi:10.1016/j.neuron.2018.02.023 (2018). 26 Meyer, A. F., O'Keefe, J. & Poort, J. Two Distinct Types of Eye-H\",\n",
       " 'ead Coupling in Freely Moving Mice. Curr Biol 30, 2116-2130 e2116, doi:10.1016/j.cub.2020.04.042 (2020). 27 Zahler, S. H., Taylor, D. E., Wong, J. Y., Adams, J. M. & Feinberg, E. H. Superior colliculu',\n",
       " 's drives stimulus-evoked directionally biased saccades and attempted head movements in head-fixed mice. Elife 10, doi:10.7554/eLife.73081 (2021). 28 Hoy, J. L., Yavorska, I., Wehr, M. & Niell, C. M. V',\n",
       " 'ision Drives Accurate Approach Behavior during Prey Capture in Laboratory Mice. Curr Biol 26, 3046-3052, doi:10.1016/j.cub.2016.09.009 (2016). 29 Dayan, P. & Berridge, K. C. Model-based and model-free',\n",
       " ' Pavlovian reward learning: revaluation, revision, and revelation. Cogn Affect Behav Neurosci 14, 473-492, doi:10.3758/s13415-014-0277-8 (2014). 30 Gire, D. H., Kapoor, V., Arrighi-Allisan, A., Semina',\n",
       " 'ra, A. & Murthy, V. N. Mice Develop Efficient Strategies for Foraging and Navigation Using Complex Natural Stimuli. Curr Biol 26, 1261-1273, doi:10.1016/j.cub.2016.03.040 (2016). 31 Kirkels, L. et al.',\n",
       " ' The opto-locomotor reflex as a tool to measure sensitivity to moving random dot patterns in mice. Sci Rep 8, 7710, doi:10.1038/s41598-018-25844-4 (2018). 32 Aru, J., Suzuki, M. & Larkum, M. E. Cellul',\n",
       " 'ar Mechanisms of Conscious Processing. Trends Cogn Sci 24, 814-825, doi:10.1016/j.tics.2020.07.006 (2020). 33 Rao, R. P. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpret',\n",
       " 'ation of some extra-classical receptive-field effects. Nat Neurosci 2, 79-87, doi:10.1038/4580 (1999).      \\n2022 Rancz Lab Projects    CONFIDENTIAL 6 34 Boyd, A. M., Sturgill, J. F., Poo, C. & Isaacs',\n",
       " 'on, J. S. Cortical feedback control of olfactory bulb circuits. Neuron 76, 1161-1174, doi:10.1016/j.neuron.2012.10.020 (2012). 35 Yu, A. J. & Dayan, P. Uncertainty, neuromodulation, and attention. Neu',\n",
       " 'ron 46, 681-692, doi:10.1016/j.neuron.2005.04.026 (2005). 36 Adams, R. A., Stephan, K. E., Brown, H. R., Frith, C. D. & Friston, K. J. The computational anatomy of psychosis. Front Psychiatry 4, 47, d',\n",
       " 'oi:10.3389/fpsyt.2013.00047 (2013). 37 Sterzer, P. et al. The Predictive Coding Account of Psychosis. Biol Psychiatry 84, 634-643, doi:10.1016/j.biopsych.2018.05.015 (2018). 38 Kometer, M., Schmidt, A',\n",
       " '., Jancke, L. & Vollenweider, F. X. Activation of serotonin 2A receptors underlies the psilocybin-induced effects on alpha oscillations, N170 visual-evoked potentials, and visual hallucinations. J Neu',\n",
       " 'rosci 33, 10544-10551, doi:10.1523/JNEUROSCI.3007-12.2013 (2013). 39 Seillier, L. et al. Serotonin Decreases the Gain of Visual Responses in Awake Macaque V1. J Neurosci 37, 11390-11405, doi:10.1523/J',\n",
       " 'NEUROSCI.1339-17.2017 (2017). 40 Michaiel, A. M., Parker, P. R. L. & Niell, C. M. A Hallucinogenic Serotonin-2A Receptor Agonist Reduces Visual Response Gain and Alters Temporal Dynamics in Mouse V1. ',\n",
       " 'Cell Rep 26, 3475-3483 e3474, doi:10.1016/j.celrep.2019.02.104 (2019). 41 Azimi, Z. et al. Separable gain control of ongoing and evoked activity in the visual cortex by serotonergic input. Elife 9, do',\n",
       " 'i:10.7554/eLife.53552 (2020). 42 Moreau, A. W., Amar, M., Le Roux, N., Morel, N. & Fossier, P. Serotoninergic fine-tuning of the excitation-inhibition balance in rat visual cortical networks. Cereb Co']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313da24-a7b5-4016-96ab-7072c49a8b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ba4675-e649-4d94-b0f0-43a803a5f8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ikharitonov/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883d6de7-d6c1-4fb5-ae70-9f44fc5aad2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_folder = '../../disk2/hugghingface_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0148252-ca9e-40b0-8f00-9cff5194ede3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# embedding chunks\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", cache_folder=cache_folder)\n",
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd59e67-b5cf-4045-b37e-941dd1ad5a51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c05aa-79c5-496c-9a8c-d80d90661566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f29410f-d493-445b-8450-ebed04ce73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a \"database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bba06a1-7406-43b3-9529-39439762da33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_id = -1\n",
    "# temp_list_chunks = []\n",
    "# temp_list_embeddings = []\n",
    "test_list = []\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    current_id+=1\n",
    "    temp_dict = {'id': current_id, 'name': pdf_path.name, 'content': chunks[i]}\n",
    "    for j in range(embeddings.shape[1]):\n",
    "        temp_dict[f'dim_{j}'] =  embeddings[i, j]\n",
    "    test_list.append(temp_dict)\n",
    "    \n",
    "pd.DataFrame(test_list).to_csv('../../disk1/vector_database.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65736ecc-cffe-4e4e-a1c6-9da294b18a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../disk1/vector_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbfba367-f6fc-450b-894a-1d00f98f11ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding_array(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return [df.iloc[i].values[0] for i in range(df.shape[0])], [df.iloc[i].values[2] for i in range(df.shape[0])], np.vstack([df.iloc[i].values[3:] for i in range(df.shape[0])]).astype(np.float32)\n",
    "\n",
    "loaded_ids, loaded_chunks, loaded_embeddings = get_embedding_array('../../disk1/vector_database.csv')\n",
    "loaded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f210f-9668-4381-ae7f-0c6535bc59f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40a9eb9d-d7a8-4248-a3d8-1021166fa20f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# semantic search\n",
    "def get_similar_results(query, embeddings, chunks, num_results=5):\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=num_results)\n",
    "    return list(np.array(chunks)[top_results.indices.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f4504a-1717-404c-a48e-dfaa3bf0e76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Next, we will develop a learned behavioural task involving eye-movements. The innate eye-movements we will investigate in Aim 2 consist of a slow pursuit phase followed by a fast “reset” saccade (Fig\n",
      "\n",
      " focus on eye-movements stabilizing the retinal image during self- and external movement. In particular, the optokinetic reflex driven by external motion has been shown to be modulated by the visual c\n",
      "\n",
      "ure 2C). However, these reflexive saccades are not the only fast gaze shifts mice can do. Increasing evidence supports the presence of stimulus driven, fast targeted eye-movements in mice26,27. Simila\n",
      "\n",
      "will employ a step-wise development. 1. Directed gaze shifts towards a stationary visual target.  2. Navigation to a stationary visual target using the vestibular platform in closed-loop (i.e. the rot\n",
      "\n",
      "rly, slow pursuit eye movements driven by the movement of the whole visual field can likely be modulated by top-down cortical activity to focus on target objects. As mice have been shown capable of co\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n'.join(get_similar_results('eye movements', loaded_embeddings, loaded_chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c1b77-5120-43b6-9a6c-d359e8dfb630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c864a5be-8378-4033-8f85-514e558eec4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Llama local launch\n",
    "\n",
    "# https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/\n",
    "# https://huggingface.co/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline\n",
    "\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "\n",
    "model_dir = \"../../disk1/llama_weights/llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "\n",
    "# tokenizer_dir = '../../disk1/llama_weights/tokenizer.model'\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dab57d8-01d7-4b66-bf87-1378034f10a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype='auto', device='cuda:0')\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype='auto', device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "907301a3-7856-48fe-a575-7936271b6a87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of eye movement relfexes exist in humans?\n",
      "There are several types of eye movement reflexes in humans, including:\n",
      "1. Pursuit reflex: This reflex causes the eyes to move in a smooth, continuous manner to track a moving object.\n",
      "2. Saccadic reflex: This reflex causes the eyes to make quick, jerky movements (saccades) to shift the gaze between different points in the visual field.\n",
      "3. Fixational reflex: This reflex causes the eyes to make small adjustments to the point of gaze in order to maintain a steady fixation on a particular point or object.\n",
      "4. Smooth pursuit reflex: This reflex causes the eyes to move in a smooth, continuous manner to track a moving object, but without the quick, jerky movements of the saccadic reflex.\n",
      "5. Optokinetic reflex: This reflex causes the eyes to move in response to a moving visual stimulus, such as a spinning wheel.\n",
      "6. Vestibulo-ocular reflex: This reflex causes the eyes to move in response to changes in the position and balance of the head, and helps to maintain steady vision during head movements.\n",
      "7. Gaze evoked reflex: This reflex causes the eyes to move in response to a visual stimulus, such as looking at a face or object.\n",
      "8. Reflexive eye movements in response to auditory stimuli: This reflex causes the eyes to move in response to sounds, such as looking towards the source of a sound.\n",
      "9. Reflexive eye movements in response to somatosensory stimuli: This reflex causes the eyes to move in response to touch or other sensory stimuli, such as looking towards a touched body part.\n",
      "These are the main types of eye movement reflexes in humans,\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline('What kind of eye movement relfexes exist in humans?\\n', do_sample=True,\n",
    "top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=400)\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206dd83-9b5d-4c96-8da7-75ed8629ec7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jh_llms]",
   "language": "python",
   "name": "conda-env-jh_llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
