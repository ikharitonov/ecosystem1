{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56354de1-be6b-411d-81c8-93a7517f3d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f23b7c-02a4-439b-a1cd-ebfbbc38ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = '../../pdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd5ea2c8-486a-4d46-a27b-8366e9a1efa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter-ikharitonov/code/ecosystem1/../../pdfs/Projects_to_candidates.pdf')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = Path(pdf_folder) / os.listdir(pdf_folder)[2]\n",
    "pdf_path.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f3f99fd6-55bc-4815-aba5-244e4d3669aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grobid_url = 'http://localhost:8070/api/processFulltextDocument'\n",
    "# with open(pdf_path, 'rb') as file:\n",
    "#     files = {'input': ('first', file, 'application/pdf')}\n",
    "#     response = requests.post(grobid_url, files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8467c53-d461-4ed1-a931-a18b5a413046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://grobid.readthedocs.io/en/latest/Grobid-service/#apiprocessfulltextdocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fcb926c-e94e-4ff8-8f74-56cba4eb2ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e466fadd-ae2e-4c49-a8fc-5fc78ede6afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(response.text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2efbde3-68b4-4ed8-91ed-ac52abc6a648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root = ET.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a53211fb-f255-42b8-b204-e976c63a2f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction',\n",
       " 'To understand how the brain works, we need hypotheses about what it is doing. Historically, the brain was treated as feedforward processing device, based initially on the connectivity diagram of cortical visual areas compiled in 1991 by Felleman and Van Essen 1 . One alternative framework recently gaining prominence is the so-called predictive processing theory ',\n",
       " 'Work Package 1. The role of top-down signalling in behaviour',\n",
       " 'This work package builds on our previous work in identifying top-down inputs to L5PNs and developing the vestibulo-visual platform. We will focus on the contribution of RSP, ACA and ORB to the functioning of V2M. The main objectives are: to further develop the experimental setup, to establish innate and learned behavioural tasks, and to record and interfere with neuronal activity during task acquisition and execution.',\n",
       " 'Aim 1. Vestibulo-visual platform development',\n",
       " 'The current working prototype of the vestibulo-visual platform (Figure ',\n",
       " 'Aim 2. Simple behaviour',\n",
       " 'We will first study the effects of top-down inputs using an innate behaviour, which does not require lengthy task acquisition. We will focus on eye-movements stabilizing the retinal image during self-and external movement. In particular, the optokinetic reflex driven by external motion has been shown to be modulated by the visual cortex ',\n",
       " 'Aim 3. Complex behaviour',\n",
       " 'Next, we will develop a learned behavioural task involving eye-movements. The innate eye-movements we will investigate in Aim 2 consist of a slow pursuit phase followed by a fast \"reset\" saccade (Figure ',\n",
       " 'Animals will be kept under a water restriction regime and task acquisition will be facilitated by water rewards. Probabilistic rewards will be used to engage the orbitofrontal cortex ',\n",
       " 'We will follow the same recording and optogenetic interference strategies describe in Aim 2 using acute recordings in trained animals. A future expansion of this research project will use chronic recordings from relevant top-down areas during task acquisition to follow the development of internal models during learning.',\n",
       " 'Work Package 2: Inhibitory and neuromodulatory mechanisms for predictive processing in V2M',\n",
       " 'There are multiple mechanisms suggested for integrating bottom-up and top-down inputs at the level of single neurons 32 and the local network 2 . To uncover and understand the mechanical substrates of predictive processing, we will expand our previous work on subcellular input localization and single-cell synaptic integration. We will focus on the most powerful bottom-up (local input from L2/3) and top-down inputs (RSP, ACA, ORB).',\n",
       " 'Aim 1: The contribution of feedforward inhibition to predictive processing',\n",
       " 'It has been suggested that prediction errors are computed in two separate circuits, signalling either more or less input than predicted ',\n",
       " 'Aim 2. The contribution of serotonergic neuromodulation to predictive processing',\n",
       " 'Neuromodulator systems can alter the balance of top-down versus bottom-up influence ',\n",
       " 'Figure 1 .',\n",
       " '1',\n",
       " 'Figure 1. Vestibular platform. (A) High-level I/O, (B) Algorithmic implementation and (C) Device, CAD model (left) and photograph (right).',\n",
       " None,\n",
       " 'Figure 2 .',\n",
       " '2',\n",
       " 'Figure 2. Eye-movement recording and visual virtual reality on the vestibular platform. A. 4 monitors surround the vestibular platform to create a virtual drum (i.e., moving gratings) to evoke a combination of optokinetic and vestibuloocular reflexes while the animal is rotated. B. Eye-movements are monitored through a camera and analysed using DeepLabCut. C. The optogenetic interference with a few hundred L5PNs in V2M results in disorganized eyemovements during combined vestibular and visual stimulation. D. Movie frame showing a mouse navigating on the vestibular platform. Importantly, visual virtual reality is combined with self-generated rotation and the 2D trajectory of the animal is shown by the red trace.',\n",
       " None]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_of_text = []\n",
    "\n",
    "for i in root[1][0]:\n",
    "    # print(i.tag)\n",
    "    for j in i:\n",
    "        # print('\\t',j.tag)\n",
    "        # if j.tag.split('}')[1] == 'p':\n",
    "        chunks_of_text.append(j.text)\n",
    "chunks_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "949c6f3e-8614-4d2f-95bd-6d90d9ef0ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction',\n",
       " 'To understand how the brain works, we need hypotheses about what it is doing. Historically, the brain was treated as feedforward processing device, based initially on the connectivity diagram of cortical visual areas compiled in 1991 by Felleman and Van Essen 1 . One alternative framework recently gaining prominence is the so-called predictive processing theory ',\n",
       " 'Work Package 1. The role of top-down signalling in behaviour',\n",
       " 'This work package builds on our previous work in identifying top-down inputs to L5PNs and developing the vestibulo-visual platform. We will focus on the contribution of RSP, ACA and ORB to the functioning of V2M. The main objectives are: to further develop the experimental setup, to establish innate and learned behavioural tasks, and to record and interfere with neuronal activity during task acquisition and execution.',\n",
       " 'Aim 1. Vestibulo-visual platform development',\n",
       " 'The current working prototype of the vestibulo-visual platform (Figure ',\n",
       " 'Aim 2. Simple behaviour',\n",
       " 'We will first study the effects of top-down inputs using an innate behaviour, which does not require lengthy task acquisition. We will focus on eye-movements stabilizing the retinal image during self-and external movement. In particular, the optokinetic reflex driven by external motion has been shown to be modulated by the visual cortex ',\n",
       " 'Aim 3. Complex behaviour',\n",
       " 'Next, we will develop a learned behavioural task involving eye-movements. The innate eye-movements we will investigate in Aim 2 consist of a slow pursuit phase followed by a fast \"reset\" saccade (Figure ',\n",
       " 'Animals will be kept under a water restriction regime and task acquisition will be facilitated by water rewards. Probabilistic rewards will be used to engage the orbitofrontal cortex ',\n",
       " 'We will follow the same recording and optogenetic interference strategies describe in Aim 2 using acute recordings in trained animals. A future expansion of this research project will use chronic recordings from relevant top-down areas during task acquisition to follow the development of internal models during learning.',\n",
       " 'Work Package 2: Inhibitory and neuromodulatory mechanisms for predictive processing in V2M',\n",
       " 'There are multiple mechanisms suggested for integrating bottom-up and top-down inputs at the level of single neurons 32 and the local network 2 . To uncover and understand the mechanical substrates of predictive processing, we will expand our previous work on subcellular input localization and single-cell synaptic integration. We will focus on the most powerful bottom-up (local input from L2/3) and top-down inputs (RSP, ACA, ORB).',\n",
       " 'Aim 1: The contribution of feedforward inhibition to predictive processing',\n",
       " 'It has been suggested that prediction errors are computed in two separate circuits, signalling either more or less input than predicted ',\n",
       " 'Aim 2. The contribution of serotonergic neuromodulation to predictive processing',\n",
       " 'Neuromodulator systems can alter the balance of top-down versus bottom-up influence ',\n",
       " 'Figure 1 .',\n",
       " '1',\n",
       " 'Figure 1. Vestibular platform. (A) High-level I/O, (B) Algorithmic implementation and (C) Device, CAD model (left) and photograph (right).',\n",
       " None,\n",
       " 'Figure 2 .',\n",
       " '2',\n",
       " 'Figure 2. Eye-movement recording and visual virtual reality on the vestibular platform. A. 4 monitors surround the vestibular platform to create a virtual drum (i.e., moving gratings) to evoke a combination of optokinetic and vestibuloocular reflexes while the animal is rotated. B. Eye-movements are monitored through a camera and analysed using DeepLabCut. C. The optogenetic interference with a few hundred L5PNs in V2M results in disorganized eyemovements during combined vestibular and visual stimulation. D. Movie frame showing a mouse navigating on the vestibular platform. Importantly, visual virtual reality is combined with self-generated rotation and the 2D trajectory of the animal is shown by the red trace.',\n",
       " None]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_of_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdbc97-01f0-43b3-8ad1-9cbedc7e6fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
